{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# WeightMapper Complete Example\n",
    "\n",
    "This notebook provides a comprehensive demonstration of the `WeightMapper` class, showcasing all its features for mapping weights between different model architectures.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `WeightMapper` helps transfer learned weights from one model architecture to another, even when layer names have changed. This is useful for:\n",
    "\n",
    "- **Model refactoring**: Renaming layers while preserving learned weights\n",
    "- **Architecture migrations**: Moving between different model structures\n",
    "- **Transfer learning**: Adapting pre-trained models to new architectures\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Group-based parameter mapping**: Parameters are organized into groups (e.g., conv layers with weight and bias)\n",
    "2. **Hierarchical structure extraction**: Parent-child relationships in model architecture are preserved\n",
    "3. **Batch normalization support**: Handles BN layers with buffers (running_mean, running_var)\n",
    "4. **Multiple mapping strategies**: Conservative, shape-only, and best-match strategies\n",
    "5. **State dict mapping**: Work with checkpoints when original model code is unavailable\n",
    "6. **Confidence scores**: Evaluate mapping quality with scoring metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from lit_wsl.models.checkpoint import load_checkpoint_as_dict\n",
    "from lit_wsl.models.weight_mapper import WeightMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic Weight Mapping\n",
    "\n",
    "Let's start by defining two similar models with different naming conventions and demonstrate basic mapping functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldModel(nn.Module):\n",
    "    \"\"\"Original model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewModel(nn.Module):\n",
    "    \"\"\"New model architecture with renamed layers.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "old_model = OldModel()\n",
    "new_model = NewModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper = WeightMapper(old_model, new_model)\n",
    "\n",
    "# Generate mapping\n",
    "mapping = mapper.suggest_mapping(threshold=0.5)\n",
    "\n",
    "# Print analysis\n",
    "print(\"Weight Mapping Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "mapper.print_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated mapping\n",
    "print(\"\\nGenerated Mapping Dictionary:\")\n",
    "print(\"=\" * 80)\n",
    "for source, target in mapping.items():\n",
    "    print(f\"{source} -> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Group-Based Parameter Mapping\n",
    "\n",
    "Parameters are organized into groups where all related parameters (e.g., weight and bias) from the same module are mapped together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceModel(nn.Module):\n",
    "    \"\"\"Source model with a specific structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    \"\"\"Target model with renamed layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Renamed layers\n",
    "        self.encoder_conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.encoder_norm1 = nn.BatchNorm2d(64)\n",
    "        self.encoder_conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.encoder_norm2 = nn.BatchNorm2d(128)\n",
    "        self.classifier = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_conv1(x)\n",
    "        x = self.encoder_norm1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.encoder_conv2(x)\n",
    "        x = self.encoder_norm2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Group-Based Parameter Mapping Demonstration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models\n",
    "source = SourceModel()\n",
    "target = TargetModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper_groups = WeightMapper(source, target)\n",
    "\n",
    "print(f\"\\nSource model has {len(mapper_groups.source_params)} parameters\")\n",
    "print(f\"Target model has {len(mapper_groups.target_params)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show source parameter groups\n",
    "print(f\"\\nSource model has {len(mapper_groups.source_groups)} parameter groups:\")\n",
    "for path, group in sorted(mapper_groups.source_groups.items()):\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"  {path:30} -> {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show target parameter groups\n",
    "print(f\"\\nTarget model has {len(mapper_groups.target_groups)} parameter groups:\")\n",
    "for path, group in sorted(mapper_groups.target_groups.items()):\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"  {path:30} -> {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest mapping\n",
    "mapping_groups = mapper_groups.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(f\"\\nMapping results: {len(mapping_groups)} parameters mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show group mappings\n",
    "if mapper_groups._group_mapping and mapper_groups._group_scores:  # type: ignore[attr-defined]\n",
    "    print(f\"\\nGroup mappings ({len(mapper_groups._group_mapping)} groups):\")  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_groups._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        score = mapper_groups._group_scores[source_path]  # type: ignore[attr-defined]\n",
    "        source_group = mapper_groups.source_groups[source_path]\n",
    "        param_types = sorted(source_group.param_types)\n",
    "        print(f\"  {source_path:30} -> {target_path:30} (score: {score:.3f})\")\n",
    "        print(f\"    Parameters in group: {param_types}\")\n",
    "\n",
    "        # Show individual parameter mappings for this group\n",
    "        for param_type in param_types:\n",
    "            source_param = source_group.params[param_type]\n",
    "            target_param = mapper_groups.target_groups[target_path].params[param_type]\n",
    "            print(f\"      {source_param.name:45} -> {target_param.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all parameters in a group are mapped together\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Verification: All parameters in a group are mapped together\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if mapper_groups._group_mapping:  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_groups._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        source_group = mapper_groups.source_groups[source_path]\n",
    "        target_group = mapper_groups.target_groups[target_path]\n",
    "\n",
    "        # Check that all param types in source are in target\n",
    "        source_types = set(source_group.param_types)\n",
    "        target_types = set(target_group.param_types)\n",
    "\n",
    "        if source_types == target_types:\n",
    "            print(f\"✓ Group {source_path:30} -> {target_path:30}\")\n",
    "            print(f\"  All {len(source_types)} parameters mapped together: {sorted(source_types)}\")\n",
    "        else:\n",
    "            print(f\"✗ ERROR: Group {source_path} has mismatched parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Hierarchical Structure Extraction\n",
    "\n",
    "The WeightMapper extracts and uses hierarchical structure from models to improve parameter mapping, preserving parent-child relationships in the model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSourceModel(nn.Module):\n",
    "    \"\"\"Source model with deep nested structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Deep nested structure\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "        )\n",
    "        self.head = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepTargetModel(nn.Module):\n",
    "    \"\"\"Target model with renamed but similar structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Similar structure, different names\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hierarchy(node, indent=0, max_depth=3):\n",
    "    \"\"\"Visualize the module hierarchy tree.\"\"\"\n",
    "    if indent > max_depth:\n",
    "        return\n",
    "\n",
    "    prefix = \"  \" * indent\n",
    "    param_info = \"\"\n",
    "    if node.parameter_group:\n",
    "        param_types = sorted(node.parameter_group.param_types)\n",
    "        param_info = f\" [{', '.join(param_types)}]\"\n",
    "\n",
    "    if node.full_path:\n",
    "        print(f\"{prefix}└─ {node.name}{param_info}\")\n",
    "    else:\n",
    "        print(f\"{prefix}<root>\")\n",
    "\n",
    "    for child in node.children.values():\n",
    "        visualize_hierarchy(child, indent + 1, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Hierarchical Structure Extraction Demonstration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models\n",
    "deep_source = DeepSourceModel()\n",
    "deep_target = DeepTargetModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper_hier = WeightMapper(deep_source, deep_target)\n",
    "\n",
    "print(f\"\\nSource model has {len(mapper_hier.source_params)} parameters\")\n",
    "print(f\"Target model has {len(mapper_hier.target_params)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSource model hierarchy:\")\n",
    "visualize_hierarchy(mapper_hier.source_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTarget model hierarchy:\")\n",
    "visualize_hierarchy(mapper_hier.target_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameter groups organized by hierarchy\n",
    "print(\"\\nSource parameter groups (organized by hierarchy):\")\n",
    "print(f\"Total groups: {len(mapper_hier.source_groups)}\")\n",
    "for path in sorted(mapper_hier.source_groups.keys(), key=lambda x: (x.count(\".\"), x)):\n",
    "    group = mapper_hier.source_groups[path]\n",
    "    indent = \"  \" * path.count(\".\")\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"{indent}{path}: {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mapping\n",
    "mapping_hier = mapper_hier.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(\"\\nMapping results:\")\n",
    "print(f\"  Total parameters mapped: {len(mapping_hier)}\")\n",
    "print(f\"  Coverage: {len(mapping_hier) / len(mapper_hier.source_params) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how hierarchical context improves matching\n",
    "if mapper_hier._hierarchy_context and mapper_hier._group_scores:  # type: ignore[attr-defined]\n",
    "    print(\"\\nHierarchical Context Impact:\")\n",
    "    print(f\"{'Source Path':<35} -> {'Target Path':<35} {'Score':>6} {'Context':>8}\")\n",
    "    print(\"-\" * 88)\n",
    "    for source_path in sorted(mapper_hier._group_mapping.keys(), key=lambda x: (x.count(\".\"), x)):  # type: ignore[attr-defined]\n",
    "        target_path = mapper_hier._group_mapping[source_path]  # type: ignore[attr-defined]\n",
    "        score = mapper_hier._group_scores[source_path]  # type: ignore[attr-defined]\n",
    "        context = mapper_hier._hierarchy_context[source_path]  # type: ignore[attr-defined]\n",
    "        print(f\"{source_path:<35} -> {target_path:<35} {score:>6.3f} {context:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify parent-child relationships\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Parent-Child Relationship Verification\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nVerifying that child modules are mapped consistently with their parents:\")\n",
    "\n",
    "for source_path, target_path in sorted(mapper_hier._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "    source_parts = source_path.split(\".\")\n",
    "    target_parts = target_path.split(\".\")\n",
    "\n",
    "    if len(source_parts) > 1:\n",
    "        # Check parent mapping\n",
    "        source_parent = \".\".join(source_parts[:-1])\n",
    "        target_parent = \".\".join(target_parts[:-1])\n",
    "\n",
    "        if source_parent in mapper_hier._group_mapping:  # type: ignore[attr-defined]\n",
    "            mapped_target_parent = mapper_hier._group_mapping[source_parent]  # type: ignore[attr-defined]\n",
    "            if mapped_target_parent == target_parent:\n",
    "                print(f\"✓ {source_path:<30} -> {target_path:<30} (parent: {source_parent})\")\n",
    "            else:\n",
    "                print(f\"⚠ {source_path:<30} -> {target_path:<30} (parent mismatch!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Batch Normalization Support\n",
    "\n",
    "Demonstrate that group-based mapping works correctly with batch normalization layers, ensuring that weight, bias, running_mean, and running_var are all mapped together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithBNBuffers(nn.Module):\n",
    "    \"\"\"Model with batch normalization that has running_mean and running_var.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize models\n",
    "source_bn = ModelWithBNBuffers()\n",
    "target_bn = ModelWithBNBuffers()\n",
    "\n",
    "# Initialize models with different values\n",
    "with torch.no_grad():\n",
    "    for p in source_bn.parameters():\n",
    "        p.fill_(1.0)\n",
    "    for p in target_bn.parameters():\n",
    "        p.fill_(0.0)\n",
    "\n",
    "mapper_bn = WeightMapper(source_bn, target_bn)\n",
    "\n",
    "# Check that parameters are grouped correctly\n",
    "print(\"\\nParameter Groups (with BatchNorm buffers):\")\n",
    "for path, group in sorted(mapper_bn.source_groups.items()):\n",
    "    print(f\"  {path}: {sorted(group.param_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mapping\n",
    "mapping_bn = mapper_bn.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(f\"\\nTotal parameters mapped: {len(mapping_bn)}\")\n",
    "print(f\"Total groups: {len(mapper_bn._group_mapping) if mapper_bn._group_mapping else 0}\")  # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that for each module, all its parameters are mapped together\n",
    "print(\"\\nGroup Mappings (verifying BatchNorm buffers):\")\n",
    "if mapper_bn._group_mapping:  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_bn._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        source_group = mapper_bn.source_groups[source_path]\n",
    "        target_group = mapper_bn.target_groups[target_path]\n",
    "\n",
    "        print(f\"\\n  {source_path} -> {target_path}\")\n",
    "        print(f\"    Source params: {sorted(source_group.param_types)}\")\n",
    "        print(f\"    Target params: {sorted(target_group.param_types)}\")\n",
    "\n",
    "        # Verify all param types match\n",
    "        if source_group.param_types != target_group.param_types:\n",
    "            raise ValueError(f\"Mismatch in param types for {source_path}\")\n",
    "\n",
    "        # Show individual mappings\n",
    "        for param_type in sorted(source_group.param_types):\n",
    "            source_param = source_group.params[param_type]\n",
    "            target_param = target_group.params[param_type]\n",
    "            print(f\"      {source_param.name} -> {target_param.name}\")\n",
    "\n",
    "print(\"\\n✓ All BatchNorm parameters are grouped and mapped together correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Multiple Mapping Strategies\n",
    "\n",
    "The WeightMapper supports different strategies for different use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the models from Part 1\n",
    "test_old = OldModel()\n",
    "test_new = NewModel()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Comparing Different Mapping Strategies\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Conservative (higher threshold, more confident matches)\n",
    "mapper_conservative = WeightMapper(test_old, test_new)\n",
    "mapping_conservative = mapper_conservative.suggest_mapping(threshold=0.7, strategy=\"conservative\")\n",
    "\n",
    "print(f\"\\n1. Conservative strategy found {len(mapping_conservative)} matches\")\n",
    "print(\"   (Uses higher threshold for more confident matches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Shape-only (ignores names, only considers shapes)\n",
    "mapper_shape = WeightMapper(test_old, test_new)\n",
    "mapping_shape = mapper_shape.suggest_mapping(strategy=\"shape_only\")\n",
    "\n",
    "print(f\"\\n2. Shape-only strategy found {len(mapping_shape)} matches\")\n",
    "print(\"   (Considers only parameter shapes, ignoring names)\")\n",
    "print(\"\\nFirst 5 mappings:\")\n",
    "for source, target in list(mapping_shape.items())[:5]:\n",
    "    source_shape = mapper_shape.source_params[source].shape\n",
    "    print(f\"   {source} ({source_shape}) -> {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Best match (default, balanced approach)\n",
    "mapper_best = WeightMapper(test_old, test_new)\n",
    "mapping_best = mapper_best.suggest_mapping(threshold=0.5, strategy=\"best_match\")\n",
    "\n",
    "print(f\"\\n3. Best-match strategy found {len(mapping_best)} matches\")\n",
    "print(\"   (Balanced approach considering both names and shapes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Confidence Scores and Analysis\n",
    "\n",
    "View mappings sorted by confidence scores to identify the most reliable matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_scores = WeightMapper(test_old, test_new)\n",
    "mapper_scores.suggest_mapping()\n",
    "\n",
    "# Get mappings with scores\n",
    "mappings_with_scores = mapper_scores.get_mapping_with_scores()\n",
    "\n",
    "# Sort by score\n",
    "mappings_with_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Top 10 mappings by confidence score:\")\n",
    "print(f\"{'Source':<40} {'Target':<40} {'Score':>8}\")\n",
    "print(\"-\" * 90)\n",
    "for source, target, score in mappings_with_scores[:10]:\n",
    "    print(f\"{source:<40} {target:<40} {score:>7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed analysis\n",
    "print(\"\\n\")\n",
    "mapper_scores.print_analysis(top_n=20, show_unmatched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Export Mapping Report\n",
    "\n",
    "Export the mapping analysis to a JSON file for documentation and future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_export = WeightMapper(test_old, test_new)\n",
    "mapper_export.suggest_mapping()\n",
    "\n",
    "# Export report\n",
    "output_path = \"/tmp/weight_mapping_report.json\"  # noqa: S108\n",
    "mapper_export.export_mapping_report(output_path)\n",
    "\n",
    "print(f\"Report exported successfully to {output_path}\")\n",
    "\n",
    "# Display the report content\n",
    "with Path(output_path).open() as f:\n",
    "    report = json.load(f)\n",
    "    print(\"\\nReport summary:\")\n",
    "    print(f\"  Total mappings: {len(report.get('mapping', {}))}\")\n",
    "    print(f\"  Unmapped source params: {len(report.get('unmapped_source', []))}\")\n",
    "    print(f\"  Unmapped target params: {len(report.get('unmapped_target', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: State Dict Mapping (Checkpoint Loading)\n",
    "\n",
    "This section demonstrates how to use `WeightMapper` when you only have a checkpoint file (state dict) without access to the original model code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldModelV2(nn.Module):\n",
    "    \"\"\"Original model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "class NewModelV2(nn.Module):\n",
    "    \"\"\"Refactored model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create and save old model checkpoint\n",
    "print(\"Creating old model and saving checkpoint...\")\n",
    "old_model_v2 = OldModelV2()\n",
    "checkpoint_path = \"/tmp/old_model_checkpoint.pth\"  # noqa: S108\n",
    "\n",
    "# Save as a typical PyTorch checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": old_model_v2.state_dict(),\n",
    "        \"epoch\": 42,\n",
    "        \"optimizer_state\": {},  # Would normally have optimizer state\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "print(f\"✓ Saved checkpoint to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load checkpoint without original model code\n",
    "print(\"Loading checkpoint (without old model code)...\")\n",
    "checkpoint = load_checkpoint_as_dict(checkpoint_path)\n",
    "old_weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "print(f\"✓ Loaded {len(old_weights)} parameters from checkpoint\")\n",
    "print(f\"  Sample keys: {list(old_weights.keys())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create new model and mapper\n",
    "print(\"Creating new model architecture...\")\n",
    "new_model_v2 = NewModelV2()\n",
    "print(f\"✓ New model has {len(list(new_model_v2.parameters()))} parameters\")\n",
    "print(f\"  Sample keys: {list(new_model_v2.state_dict().keys())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapper from state dict\n",
    "print(\"Creating WeightMapper from state dict...\")\n",
    "mapper_from_dict = WeightMapper.from_state_dict(old_weights, new_model_v2)\n",
    "print(\"✓ Mapper created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate and analyze mapping\n",
    "print(\"Generating weight mapping...\")\n",
    "mapping_dict = mapper_from_dict.suggest_mapping(strategy=\"best_match\", threshold=0.5)\n",
    "print(f\"✓ Found {len(mapping_dict)} parameter mappings\")\n",
    "\n",
    "# Show analysis\n",
    "print(\"\\nMapping analysis:\")\n",
    "print(\"-\" * 80)\n",
    "mapper_from_dict.print_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply mapping and load weights\n",
    "print(\"Applying mapping to create new checkpoint...\")\n",
    "new_weights = {}\n",
    "for old_key, new_key in mapping_dict.items():\n",
    "    new_weights[new_key] = old_weights[old_key]\n",
    "\n",
    "# Load into new model\n",
    "missing, unexpected = new_model_v2.load_state_dict(new_weights, strict=False)\n",
    "print(f\"✓ Loaded {len(new_weights)} weights into new model\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"  Missing keys: {len(missing)}\")\n",
    "    print(f\"  Examples: {list(missing)[:3]}\")\n",
    "if unexpected:\n",
    "    print(f\"  Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Successfully mapped weights from old checkpoint to new model!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf32e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: New Dataclass-Based API\n",
    "\n",
    "This section demonstrates the refactored WeightMapper API that provides:\n",
    "\n",
    "1. **Dataclass-based return types** for structured access to results\n",
    "2. **Full scoring transparency** with breakdown of all components\n",
    "3. **Ability to explore all compatible group mappings**\n",
    "4. **Soft hierarchical constraints** instead of hard rejections\n",
    "5. **Enhanced filtering and analysis** capabilities\n",
    "\n",
    "The new API makes it easier to understand and debug weight mappings with comprehensive access to scoring details and match information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513d471",
   "metadata": {},
   "source": [
    "## Define Example Models\n",
    "\n",
    "We'll use models with reorganized structure to demonstrate the new API features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3306c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalModel(nn.Module):\n",
    "    \"\"\"Original model with typical architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReorganizedModel(nn.Module):\n",
    "    \"\"\"Reorganized model where components are renamed/moved.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2727a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "source_api = OriginalModel()\n",
    "target_api = ReorganizedModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper_api = WeightMapper(source_api, target_api)\n",
    "\n",
    "print(\"Models created successfully!\")\n",
    "print(f\"Source parameters: {len(list(source_api.parameters()))}\")\n",
    "print(f\"Target parameters: {len(list(target_api.parameters()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771f565",
   "metadata": {},
   "source": [
    "## 9.1 MappingResult Dataclass\n",
    "\n",
    "The `suggest_mapping()` method now returns a `MappingResult` dataclass instead of a plain dictionary. This provides structured access to the mapping and additional metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edf3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"1. NEW DATACLASS-BASED API\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = mapper_api.suggest_mapping(threshold=0.5)\n",
    "\n",
    "# Access mapping through convenience methods\n",
    "mapping_api = result.get_mapping()\n",
    "print(f\"\\nMatched {len(mapping_api)} parameters\")\n",
    "print(f\"Coverage: {result.coverage * 100:.1f}%\")\n",
    "\n",
    "# Show first few mappings\n",
    "print(\"\\nFirst 5 mappings:\")\n",
    "for source, target in list(mapping_api.items())[:5]:\n",
    "    print(f\"  {source} → {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd8668",
   "metadata": {},
   "source": [
    "## 9.2 Scoring Transparency\n",
    "\n",
    "Access detailed score breakdowns for each mapping to understand why parameters were matched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09dc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"2. SCORING TRANSPARENCY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get mapping with scores\n",
    "mappings_with_scores_api = result.get_mapping_with_scores()\n",
    "\n",
    "for source_name, (target_name, final_score) in list(mappings_with_scores_api.items())[:3]:\n",
    "    print(f\"\\n{source_name} → {target_name}\")\n",
    "    print(f\"  Final score: {final_score:.3f}\")\n",
    "\n",
    "    # Get detailed breakdown\n",
    "    breakdown = mapper_api.get_score_breakdown(source_name)\n",
    "    print(f\"  Shape score: {breakdown.shape_score:.3f}\")\n",
    "    print(f\"  Name score:  {breakdown.name_score:.3f}\")\n",
    "    print(f\"  Hierarchy:   {breakdown.hierarchy_score:.3f}\")\n",
    "\n",
    "    # Access even more detail if needed\n",
    "    if breakdown.depth_score is not None:\n",
    "        print(f\"    - Depth:   {breakdown.depth_score:.3f}\")\n",
    "    if breakdown.path_score is not None:\n",
    "        print(f\"    - Path:    {breakdown.path_score:.3f}\")\n",
    "    if breakdown.order_score is not None:\n",
    "        print(f\"    - Order:   {breakdown.order_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3eb39d",
   "metadata": {},
   "source": [
    "## 9.3 Parameter Match Details\n",
    "\n",
    "Access full information about each parameter match, including match type and any transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"3. PARAMETER MATCH DETAILS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "param_name = list(mapping_api.keys())[0]\n",
    "match_details = mapper_api.get_parameter_details(param_name)\n",
    "\n",
    "print(f\"\\nParameter: {match_details.source_name}\")\n",
    "print(f\"  Matched: {match_details.matched}\")\n",
    "print(f\"  Target: {match_details.target_name}\")\n",
    "print(f\"  Match type: {match_details.match_type}\")\n",
    "print(f\"  Score: {match_details.final_score:.3f}\")\n",
    "if match_details.transformation:\n",
    "    print(f\"  Transformation: {match_details.transformation.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf023f",
   "metadata": {},
   "source": [
    "## 9.4 Compatible Group Exploration\n",
    "\n",
    "Explore all compatible target groups for a given source group to see alternative mappings and their scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"4. COMPATIBLE GROUP EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all compatible groups for a specific source path\n",
    "print(f\"\\nAvailable source groups: {list(mapper_api.source_groups.keys())[:5]}\")\n",
    "\n",
    "# Use an actual group path\n",
    "if mapper_api.source_groups:\n",
    "    first_group = list(mapper_api.source_groups.keys())[0]\n",
    "    compatible = mapper_api.get_compatible_groups(\n",
    "        source_path=first_group,\n",
    "        threshold=0.0,  # Show all candidates\n",
    "        max_candidates=5,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCompatible target groups for '{first_group}':\")\n",
    "    if first_group in compatible:\n",
    "        for candidate in compatible[first_group][:3]:\n",
    "            print(f\"  {candidate.target_path}: {candidate.combined_score:.3f}\")\n",
    "            print(f\"    Structure: {candidate.structure_score:.3f}\")\n",
    "            print(f\"    Matched types: {candidate.param_types_matched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53817307",
   "metadata": {},
   "source": [
    "## 9.5 Unmatched Parameter Analysis\n",
    "\n",
    "Analyze which parameters couldn't be matched and understand why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"5. UNMATCHED PARAMETERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get unmatched through mapper (returns dict with 'source' and 'target' keys)\n",
    "unmatched = mapper_api.get_unmatched()\n",
    "\n",
    "if unmatched.get(\"source\"):\n",
    "    print(f\"\\nUnmatched source parameters: {len(unmatched['source'])}\")\n",
    "    for param in unmatched[\"source\"][:3]:\n",
    "        details = mapper_api.get_parameter_details(param)\n",
    "        print(f\"  {param}: {details.unmatch_reason}\")\n",
    "else:\n",
    "    print(\"\\nAll source parameters matched successfully!\")\n",
    "\n",
    "if unmatched.get(\"target\"):\n",
    "    print(f\"\\nUnmatched target parameters: {len(unmatched['target'])}\")\n",
    "    for param in unmatched[\"target\"][:3]:\n",
    "        print(f\"  {param}\")\n",
    "else:\n",
    "    print(\"All target parameters matched successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead106fc",
   "metadata": {},
   "source": [
    "## 9.6 Confidence Filtering\n",
    "\n",
    "Filter mappings by confidence score to identify high-quality matches or those that need review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f43f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"6. CONFIDENCE FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get only high-confidence matches\n",
    "high_confidence = result.filter_by_score(min_score=0.8)\n",
    "print(f\"\\nHigh confidence matches (score >= 0.8): {len(high_confidence.matched_params)}\")\n",
    "\n",
    "# Get low-confidence matches that might need review\n",
    "low_confidence = result.get_low_confidence_matches(threshold=0.6)\n",
    "print(f\"Low confidence matches (score < 0.6): {len(low_confidence)}\")\n",
    "\n",
    "if low_confidence:\n",
    "    print(\"\\nLow confidence matches (may need review):\")\n",
    "    for source, target, score in low_confidence[:3]:\n",
    "        print(f\"  {source} → {target} (score: {score:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ New API demonstration complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42101ba9",
   "metadata": {},
   "source": [
    "### Key Takeaways from the New API\n",
    "\n",
    "The dataclass-based API provides:\n",
    "\n",
    "1. **Structured Results**: `MappingResult` dataclass with typed fields and convenience methods\n",
    "2. **Score Breakdown**: Detailed scoring components (shape, name, hierarchy, depth, path, order)\n",
    "3. **Parameter Details**: Full match information including match type and transformations\n",
    "4. **Group Exploration**: View all compatible group candidates with scores\n",
    "5. **Enhanced Filtering**: Filter by confidence score to identify matches needing review\n",
    "6. **Better Debugging**: Access to unmatch reasons and detailed scoring information\n",
    "\n",
    "This makes it much easier to understand, debug, and validate weight mappings between models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664c7f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "checkpoint_file = Path(checkpoint_path)\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()\n",
    "    print(f\"Cleaned up {checkpoint_path}\")\n",
    "\n",
    "report_file = Path(\"/tmp/weight_mapping_report.json\")  # noqa: S108\n",
    "if report_file.exists():\n",
    "    report_file.unlink()\n",
    "    print(\"Cleaned up /tmp/weight_mapping_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated all key features of the `WeightMapper` class:\n",
    "\n",
    "## 1. **Basic Weight Mapping**\n",
    "\n",
    "- Map weights between models with different naming conventions\n",
    "- Automatic parameter matching based on structure similarity\n",
    "\n",
    "## 2. **Group-Based Parameter Mapping**\n",
    "\n",
    "- Parameters organized into groups (e.g., conv weight + bias)\n",
    "- All related parameters mapped together as a cohesive unit\n",
    "\n",
    "## 3. **Hierarchical Structure Extraction**\n",
    "\n",
    "- Parent-child relationships in model architecture are preserved\n",
    "- Hierarchical context improves matching accuracy\n",
    "\n",
    "## 4. **Batch Normalization Support**\n",
    "\n",
    "- Handles BN layers with all their components (weight, bias, running_mean, running_var)\n",
    "- All buffers and parameters mapped together correctly\n",
    "\n",
    "## 5. **Multiple Mapping Strategies**\n",
    "\n",
    "- **Conservative**: Higher threshold for more confident matches\n",
    "- **Shape-only**: Considers only parameter shapes, ignoring names\n",
    "- **Best-match**: Balanced approach (default)\n",
    "\n",
    "## 6. **Confidence Scores**\n",
    "\n",
    "- Quality metrics for evaluating mapping reliability\n",
    "- Detailed analysis with scoring information\n",
    "\n",
    "## 7. **Export Capabilities**\n",
    "\n",
    "- Save mapping reports as JSON for documentation\n",
    "- Review and validate mappings before applying\n",
    "\n",
    "## 8. **State Dict Mapping**\n",
    "\n",
    "- Work with checkpoints when original model code is unavailable\n",
    "- Essential for model refactoring and architecture migrations\n",
    "\n",
    "## 9. **New Dataclass-Based API**\n",
    "\n",
    "- Structured `MappingResult` dataclass with typed fields and convenience methods\n",
    "- Detailed score breakdowns (shape, name, hierarchy, depth, path, order)\n",
    "- Full parameter match details including match type and transformations\n",
    "- Compatible group exploration to view alternative mappings\n",
    "- Enhanced confidence filtering and unmatch reason analysis\n",
    "- Better debugging with comprehensive scoring information\n",
    "\n",
    "The `WeightMapper` class provides flexible and powerful tools for transferring weights between different model architectures, making it easier to refactor code while preserving learned parameters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
