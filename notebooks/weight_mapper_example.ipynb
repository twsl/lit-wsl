{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32be3b5c",
   "metadata": {},
   "source": [
    "# WeightMapper Complete Example\n",
    "\n",
    "This notebook provides a comprehensive demonstration of the `WeightMapper` class, showcasing all its features for mapping weights between different model architectures.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `WeightMapper` helps transfer learned weights from one model architecture to another, even when layer names have changed. This is useful for:\n",
    "\n",
    "- **Model refactoring**: Renaming layers while preserving learned weights\n",
    "- **Architecture migrations**: Moving between different model structures\n",
    "- **Transfer learning**: Adapting pre-trained models to new architectures\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Group-based parameter mapping**: Parameters are organized into groups (e.g., conv layers with weight and bias)\n",
    "2. **Hierarchical structure extraction**: Parent-child relationships in model architecture are preserved\n",
    "3. **Batch normalization support**: Handles BN layers with buffers (running_mean, running_var)\n",
    "4. **Multiple mapping strategies**: Conservative, shape-only, and best-match strategies\n",
    "5. **State dict mapping**: Work with checkpoints when original model code is unavailable\n",
    "6. **Confidence scores**: Evaluate mapping quality with scoring metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2787198",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b077bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from lit_wsl.models.checkpoint import load_checkpoint_as_dict\n",
    "from lit_wsl.models.weight_mapper import WeightMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8201a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic Weight Mapping\n",
    "\n",
    "Let's start by defining two similar models with different naming conventions and demonstrate basic mapping functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldModel(nn.Module):\n",
    "    \"\"\"Original model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewModel(nn.Module):\n",
    "    \"\"\"New model architecture with renamed layers.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "old_model = OldModel()\n",
    "new_model = NewModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper = WeightMapper(old_model, new_model)\n",
    "\n",
    "# Generate mapping\n",
    "mapping = mapper.suggest_mapping(threshold=0.5)\n",
    "\n",
    "# Print analysis\n",
    "print(\"Weight Mapping Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "mapper.print_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdaac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated mapping\n",
    "print(\"\\nGenerated Mapping Dictionary:\")\n",
    "print(\"=\" * 80)\n",
    "for source, target in mapping.items():\n",
    "    print(f\"{source} -> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e4cb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Group-Based Parameter Mapping\n",
    "\n",
    "Parameters are organized into groups where all related parameters (e.g., weight and bias) from the same module are mapped together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8631b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceModel(nn.Module):\n",
    "    \"\"\"Source model with a specific structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    \"\"\"Target model with renamed layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Renamed layers\n",
    "        self.encoder_conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.encoder_norm1 = nn.BatchNorm2d(64)\n",
    "        self.encoder_conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.encoder_norm2 = nn.BatchNorm2d(128)\n",
    "        self.classifier = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_conv1(x)\n",
    "        x = self.encoder_norm1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.encoder_conv2(x)\n",
    "        x = self.encoder_norm2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Group-Based Parameter Mapping Demonstration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models\n",
    "source = SourceModel()\n",
    "target = TargetModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper_groups = WeightMapper(source, target)\n",
    "\n",
    "print(f\"\\nSource model has {len(mapper_groups.source_params)} parameters\")\n",
    "print(f\"Target model has {len(mapper_groups.target_params)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show source parameter groups\n",
    "print(f\"\\nSource model has {len(mapper_groups.source_groups)} parameter groups:\")\n",
    "for path, group in sorted(mapper_groups.source_groups.items()):\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"  {path:30} -> {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288dc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show target parameter groups\n",
    "print(f\"\\nTarget model has {len(mapper_groups.target_groups)} parameter groups:\")\n",
    "for path, group in sorted(mapper_groups.target_groups.items()):\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"  {path:30} -> {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0870b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest mapping\n",
    "mapping_groups = mapper_groups.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(f\"\\nMapping results: {len(mapping_groups)} parameters mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e326db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show group mappings\n",
    "if mapper_groups._group_mapping and mapper_groups._group_scores:  # type: ignore[attr-defined]\n",
    "    print(f\"\\nGroup mappings ({len(mapper_groups._group_mapping)} groups):\")  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_groups._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        score = mapper_groups._group_scores[source_path]  # type: ignore[attr-defined]\n",
    "        source_group = mapper_groups.source_groups[source_path]\n",
    "        param_types = sorted(source_group.param_types)\n",
    "        print(f\"  {source_path:30} -> {target_path:30} (score: {score:.3f})\")\n",
    "        print(f\"    Parameters in group: {param_types}\")\n",
    "\n",
    "        # Show individual parameter mappings for this group\n",
    "        for param_type in param_types:\n",
    "            source_param = source_group.params[param_type]\n",
    "            target_param = mapper_groups.target_groups[target_path].params[param_type]\n",
    "            print(f\"      {source_param.name:45} -> {target_param.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all parameters in a group are mapped together\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Verification: All parameters in a group are mapped together\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if mapper_groups._group_mapping:  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_groups._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        source_group = mapper_groups.source_groups[source_path]\n",
    "        target_group = mapper_groups.target_groups[target_path]\n",
    "\n",
    "        # Check that all param types in source are in target\n",
    "        source_types = set(source_group.param_types)\n",
    "        target_types = set(target_group.param_types)\n",
    "\n",
    "        if source_types == target_types:\n",
    "            print(f\"✓ Group {source_path:30} -> {target_path:30}\")\n",
    "            print(f\"  All {len(source_types)} parameters mapped together: {sorted(source_types)}\")\n",
    "        else:\n",
    "            print(f\"✗ ERROR: Group {source_path} has mismatched parameters!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285af50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Hierarchical Structure Extraction\n",
    "\n",
    "The WeightMapper extracts and uses hierarchical structure from models to improve parameter mapping, preserving parent-child relationships in the model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSourceModel(nn.Module):\n",
    "    \"\"\"Source model with deep nested structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Deep nested structure\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "        )\n",
    "        self.head = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepTargetModel(nn.Module):\n",
    "    \"\"\"Target model with renamed but similar structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Similar structure, different names\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce41b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hierarchy(node, indent=0, max_depth=3):\n",
    "    \"\"\"Visualize the module hierarchy tree.\"\"\"\n",
    "    if indent > max_depth:\n",
    "        return\n",
    "\n",
    "    prefix = \"  \" * indent\n",
    "    param_info = \"\"\n",
    "    if node.parameter_group:\n",
    "        param_types = sorted(node.parameter_group.param_types)\n",
    "        param_info = f\" [{', '.join(param_types)}]\"\n",
    "\n",
    "    if node.full_path:\n",
    "        print(f\"{prefix}└─ {node.name}{param_info}\")\n",
    "    else:\n",
    "        print(f\"{prefix}<root>\")\n",
    "\n",
    "    for child in node.children.values():\n",
    "        visualize_hierarchy(child, indent + 1, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be816bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Hierarchical Structure Extraction Demonstration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models\n",
    "deep_source = DeepSourceModel()\n",
    "deep_target = DeepTargetModel()\n",
    "\n",
    "# Create mapper\n",
    "mapper_hier = WeightMapper(deep_source, deep_target)\n",
    "\n",
    "print(f\"\\nSource model has {len(mapper_hier.source_params)} parameters\")\n",
    "print(f\"Target model has {len(mapper_hier.target_params)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a22bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSource model hierarchy:\")\n",
    "visualize_hierarchy(mapper_hier.source_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTarget model hierarchy:\")\n",
    "visualize_hierarchy(mapper_hier.target_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameter groups organized by hierarchy\n",
    "print(\"\\nSource parameter groups (organized by hierarchy):\")\n",
    "print(f\"Total groups: {len(mapper_hier.source_groups)}\")\n",
    "for path in sorted(mapper_hier.source_groups.keys(), key=lambda x: (x.count(\".\"), x)):\n",
    "    group = mapper_hier.source_groups[path]\n",
    "    indent = \"  \" * path.count(\".\")\n",
    "    param_types = sorted(group.param_types)\n",
    "    print(f\"{indent}{path}: {param_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3250b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mapping\n",
    "mapping_hier = mapper_hier.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(\"\\nMapping results:\")\n",
    "print(f\"  Total parameters mapped: {len(mapping_hier)}\")\n",
    "print(f\"  Coverage: {len(mapping_hier) / len(mapper_hier.source_params) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how hierarchical context improves matching\n",
    "if mapper_hier._hierarchy_context and mapper_hier._group_scores:  # type: ignore[attr-defined]\n",
    "    print(\"\\nHierarchical Context Impact:\")\n",
    "    print(f\"{'Source Path':<35} -> {'Target Path':<35} {'Score':>6} {'Context':>8}\")\n",
    "    print(\"-\" * 88)\n",
    "    for source_path in sorted(mapper_hier._group_mapping.keys(), key=lambda x: (x.count(\".\"), x)):  # type: ignore[attr-defined]\n",
    "        target_path = mapper_hier._group_mapping[source_path]  # type: ignore[attr-defined]\n",
    "        score = mapper_hier._group_scores[source_path]  # type: ignore[attr-defined]\n",
    "        context = mapper_hier._hierarchy_context[source_path]  # type: ignore[attr-defined]\n",
    "        print(f\"{source_path:<35} -> {target_path:<35} {score:>6.3f} {context:>8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify parent-child relationships\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Parent-Child Relationship Verification\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nVerifying that child modules are mapped consistently with their parents:\")\n",
    "\n",
    "for source_path, target_path in sorted(mapper_hier._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "    source_parts = source_path.split(\".\")\n",
    "    target_parts = target_path.split(\".\")\n",
    "\n",
    "    if len(source_parts) > 1:\n",
    "        # Check parent mapping\n",
    "        source_parent = \".\".join(source_parts[:-1])\n",
    "        target_parent = \".\".join(target_parts[:-1])\n",
    "\n",
    "        if source_parent in mapper_hier._group_mapping:  # type: ignore[attr-defined]\n",
    "            mapped_target_parent = mapper_hier._group_mapping[source_parent]  # type: ignore[attr-defined]\n",
    "            if mapped_target_parent == target_parent:\n",
    "                print(f\"✓ {source_path:<30} -> {target_path:<30} (parent: {source_parent})\")\n",
    "            else:\n",
    "                print(f\"⚠ {source_path:<30} -> {target_path:<30} (parent mismatch!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9fe0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Batch Normalization Support\n",
    "\n",
    "Demonstrate that group-based mapping works correctly with batch normalization layers, ensuring that weight, bias, running_mean, and running_var are all mapped together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c54945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithBNBuffers(nn.Module):\n",
    "    \"\"\"Model with batch normalization that has running_mean and running_var.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize models\n",
    "source_bn = ModelWithBNBuffers()\n",
    "target_bn = ModelWithBNBuffers()\n",
    "\n",
    "# Initialize models with different values\n",
    "with torch.no_grad():\n",
    "    for p in source_bn.parameters():\n",
    "        p.fill_(1.0)\n",
    "    for p in target_bn.parameters():\n",
    "        p.fill_(0.0)\n",
    "\n",
    "mapper_bn = WeightMapper(source_bn, target_bn)\n",
    "\n",
    "# Check that parameters are grouped correctly\n",
    "print(\"\\nParameter Groups (with BatchNorm buffers):\")\n",
    "for path, group in sorted(mapper_bn.source_groups.items()):\n",
    "    print(f\"  {path}: {sorted(group.param_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06159ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mapping\n",
    "mapping_bn = mapper_bn.suggest_mapping(threshold=0.5)\n",
    "\n",
    "print(f\"\\nTotal parameters mapped: {len(mapping_bn)}\")\n",
    "print(f\"Total groups: {len(mapper_bn._group_mapping) if mapper_bn._group_mapping else 0}\")  # type: ignore[attr-defined]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that for each module, all its parameters are mapped together\n",
    "print(\"\\nGroup Mappings (verifying BatchNorm buffers):\")\n",
    "if mapper_bn._group_mapping:  # type: ignore[attr-defined]\n",
    "    for source_path, target_path in sorted(mapper_bn._group_mapping.items()):  # type: ignore[attr-defined]\n",
    "        source_group = mapper_bn.source_groups[source_path]\n",
    "        target_group = mapper_bn.target_groups[target_path]\n",
    "\n",
    "        print(f\"\\n  {source_path} -> {target_path}\")\n",
    "        print(f\"    Source params: {sorted(source_group.param_types)}\")\n",
    "        print(f\"    Target params: {sorted(target_group.param_types)}\")\n",
    "\n",
    "        # Verify all param types match\n",
    "        if source_group.param_types != target_group.param_types:\n",
    "            raise ValueError(f\"Mismatch in param types for {source_path}\")\n",
    "\n",
    "        # Show individual mappings\n",
    "        for param_type in sorted(source_group.param_types):\n",
    "            source_param = source_group.params[param_type]\n",
    "            target_param = target_group.params[param_type]\n",
    "            print(f\"      {source_param.name} -> {target_param.name}\")\n",
    "\n",
    "print(\"\\n✓ All BatchNorm parameters are grouped and mapped together correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9370e46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Multiple Mapping Strategies\n",
    "\n",
    "The WeightMapper supports different strategies for different use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ebe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the models from Part 1\n",
    "test_old = OldModel()\n",
    "test_new = NewModel()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Comparing Different Mapping Strategies\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Conservative (higher threshold, more confident matches)\n",
    "mapper_conservative = WeightMapper(test_old, test_new)\n",
    "mapping_conservative = mapper_conservative.suggest_mapping(threshold=0.7, strategy=\"conservative\")\n",
    "\n",
    "print(f\"\\n1. Conservative strategy found {len(mapping_conservative)} matches\")\n",
    "print(\"   (Uses higher threshold for more confident matches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24041b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Shape-only (ignores names, only considers shapes)\n",
    "mapper_shape = WeightMapper(test_old, test_new)\n",
    "mapping_shape = mapper_shape.suggest_mapping(strategy=\"shape_only\")\n",
    "\n",
    "print(f\"\\n2. Shape-only strategy found {len(mapping_shape)} matches\")\n",
    "print(\"   (Considers only parameter shapes, ignoring names)\")\n",
    "print(\"\\nFirst 5 mappings:\")\n",
    "for source, target in list(mapping_shape.items())[:5]:\n",
    "    source_shape = mapper_shape.source_params[source].shape\n",
    "    print(f\"   {source} ({source_shape}) -> {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Best match (default, balanced approach)\n",
    "mapper_best = WeightMapper(test_old, test_new)\n",
    "mapping_best = mapper_best.suggest_mapping(threshold=0.5, strategy=\"best_match\")\n",
    "\n",
    "print(f\"\\n3. Best-match strategy found {len(mapping_best)} matches\")\n",
    "print(\"   (Balanced approach considering both names and shapes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f944401",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Confidence Scores and Analysis\n",
    "\n",
    "View mappings sorted by confidence scores to identify the most reliable matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_scores = WeightMapper(test_old, test_new)\n",
    "mapper_scores.suggest_mapping()\n",
    "\n",
    "# Get mappings with scores\n",
    "mappings_with_scores = mapper_scores.get_mapping_with_scores()\n",
    "\n",
    "# Sort by score\n",
    "mappings_with_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Top 10 mappings by confidence score:\")\n",
    "print(f\"{'Source':<40} {'Target':<40} {'Score':>8}\")\n",
    "print(\"-\" * 90)\n",
    "for source, target, score in mappings_with_scores[:10]:\n",
    "    print(f\"{source:<40} {target:<40} {score:>7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed analysis\n",
    "print(\"\\n\")\n",
    "mapper_scores.print_analysis(top_n=20, show_unmatched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60da059",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Export Mapping Report\n",
    "\n",
    "Export the mapping analysis to a JSON file for documentation and future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_export = WeightMapper(test_old, test_new)\n",
    "mapper_export.suggest_mapping()\n",
    "\n",
    "# Export report\n",
    "output_path = \"/tmp/weight_mapping_report.json\"  # noqa: S108\n",
    "mapper_export.export_mapping_report(output_path)\n",
    "\n",
    "print(f\"Report exported successfully to {output_path}\")\n",
    "\n",
    "# Display the report content\n",
    "with Path(output_path).open() as f:\n",
    "    report = json.load(f)\n",
    "    print(\"\\nReport summary:\")\n",
    "    print(f\"  Total mappings: {len(report.get('mapping', {}))}\")\n",
    "    print(f\"  Unmapped source params: {len(report.get('unmapped_source', []))}\")\n",
    "    print(f\"  Unmapped target params: {len(report.get('unmapped_target', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd9fad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: State Dict Mapping (Checkpoint Loading)\n",
    "\n",
    "This section demonstrates how to use `WeightMapper` when you only have a checkpoint file (state dict) without access to the original model code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldModelV2(nn.Module):\n",
    "    \"\"\"Original model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "class NewModelV2(nn.Module):\n",
    "    \"\"\"Refactored model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create and save old model checkpoint\n",
    "print(\"Creating old model and saving checkpoint...\")\n",
    "old_model_v2 = OldModelV2()\n",
    "checkpoint_path = \"/tmp/old_model_checkpoint.pth\"  # noqa: S108\n",
    "\n",
    "# Save as a typical PyTorch checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": old_model_v2.state_dict(),\n",
    "        \"epoch\": 42,\n",
    "        \"optimizer_state\": {},  # Would normally have optimizer state\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "print(f\"✓ Saved checkpoint to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load checkpoint without original model code\n",
    "print(\"Loading checkpoint (without old model code)...\")\n",
    "checkpoint = load_checkpoint_as_dict(checkpoint_path)\n",
    "old_weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "print(f\"✓ Loaded {len(old_weights)} parameters from checkpoint\")\n",
    "print(f\"  Sample keys: {list(old_weights.keys())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create new model and mapper\n",
    "print(\"Creating new model architecture...\")\n",
    "new_model_v2 = NewModelV2()\n",
    "print(f\"✓ New model has {len(list(new_model_v2.parameters()))} parameters\")\n",
    "print(f\"  Sample keys: {list(new_model_v2.state_dict().keys())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0932d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapper from state dict\n",
    "print(\"Creating WeightMapper from state dict...\")\n",
    "mapper_from_dict = WeightMapper.from_state_dict(old_weights, new_model_v2)\n",
    "print(\"✓ Mapper created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d628f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate and analyze mapping\n",
    "print(\"Generating weight mapping...\")\n",
    "mapping_dict = mapper_from_dict.suggest_mapping(strategy=\"best_match\", threshold=0.5)\n",
    "print(f\"✓ Found {len(mapping_dict)} parameter mappings\")\n",
    "\n",
    "# Show analysis\n",
    "print(\"\\nMapping analysis:\")\n",
    "print(\"-\" * 80)\n",
    "mapper_from_dict.print_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6638ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply mapping and load weights\n",
    "print(\"Applying mapping to create new checkpoint...\")\n",
    "new_weights = {}\n",
    "for old_key, new_key in mapping_dict.items():\n",
    "    new_weights[new_key] = old_weights[old_key]\n",
    "\n",
    "# Load into new model\n",
    "missing, unexpected = new_model_v2.load_state_dict(new_weights, strict=False)\n",
    "print(f\"✓ Loaded {len(new_weights)} weights into new model\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"  Missing keys: {len(missing)}\")\n",
    "    print(f\"  Examples: {list(missing)[:3]}\")\n",
    "if unexpected:\n",
    "    print(f\"  Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Successfully mapped weights from old checkpoint to new model!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3247d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "checkpoint_file = Path(checkpoint_path)\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()\n",
    "    print(f\"Cleaned up {checkpoint_path}\")\n",
    "\n",
    "report_file = Path(\"/tmp/weight_mapping_report.json\")  # noqa: S108\n",
    "if report_file.exists():\n",
    "    report_file.unlink()\n",
    "    print(\"Cleaned up /tmp/weight_mapping_report.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b547346",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated all key features of the `WeightMapper` class:\n",
    "\n",
    "## 1. **Basic Weight Mapping**\n",
    "\n",
    "- Map weights between models with different naming conventions\n",
    "- Automatic parameter matching based on structure similarity\n",
    "\n",
    "## 2. **Group-Based Parameter Mapping**\n",
    "\n",
    "- Parameters organized into groups (e.g., conv weight + bias)\n",
    "- All related parameters mapped together as a cohesive unit\n",
    "\n",
    "## 3. **Hierarchical Structure Extraction**\n",
    "\n",
    "- Parent-child relationships in model architecture are preserved\n",
    "- Hierarchical context improves matching accuracy\n",
    "\n",
    "## 4. **Batch Normalization Support**\n",
    "\n",
    "- Handles BN layers with all their components (weight, bias, running_mean, running_var)\n",
    "- All buffers and parameters mapped together correctly\n",
    "\n",
    "## 5. **Multiple Mapping Strategies**\n",
    "\n",
    "- **Conservative**: Higher threshold for more confident matches\n",
    "- **Shape-only**: Considers only parameter shapes, ignoring names\n",
    "- **Best-match**: Balanced approach (default)\n",
    "\n",
    "## 6. **Confidence Scores**\n",
    "\n",
    "- Quality metrics for evaluating mapping reliability\n",
    "- Detailed analysis with scoring information\n",
    "\n",
    "## 7. **Export Capabilities**\n",
    "\n",
    "- Save mapping reports as JSON for documentation\n",
    "- Review and validate mappings before applying\n",
    "\n",
    "## 8. **State Dict Mapping**\n",
    "\n",
    "- Work with checkpoints when original model code is unavailable\n",
    "- Essential for model refactoring and architecture migrations\n",
    "\n",
    "The `WeightMapper` class provides flexible and powerful tools for transferring weights between different model architectures, making it easier to refactor code while preserving learned parameters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
